{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fd04231"
   },
   "source": [
    "# Chunking Strategies for RAG: Practical Implementation with LangChain and LlamaIndex\n",
    "\n",
    "This notebook explores different text chunking strategies essential for building effective Retrieval Augmented Generation (RAG) systems. We will implement and compare various methods using LangChain and LlamaIndex.\n",
    "\n",
    "\n",
    "This notebook is written to work in Google Colab. Please add you OpenAI api key in secrets for the more advanced chunkign implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6caabfea"
   },
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94a1b17a"
   },
   "outputs": [],
   "source": [
    "!pip install langchain llama-index openai chromadb ragas datasets langchain_experimental\n",
    "# Optionally: sentence-transformers, tqdm, rich, matplotlib if needed for analysis\n",
    "# !pip install sentence-transformers tqdm rich matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eac0ec9e"
   },
   "source": [
    "### 2. Load Sample Text Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "53b6a9b5"
   },
   "outputs": [],
   "source": [
    "# Replace this with your sample text document\n",
    "sample_text = \"\"\"\n",
    "Large language models (LLMs) are a type of artificial intelligence algorithm that uses deep learning techniques and massively large data sets to understand, summarize, generate and predict new content. The \"large\" in LLM refers to the vast amount of data these models are trained on — often terabytes of text and data — and the number of parameters they use to process language, which can be billions.\n",
    "\n",
    "LLMs are built on transformer architectures, a neural network architecture introduced in 2017 that processes input data in a way that allows for parallelization, making training on massive data sets feasible. This architecture uses self-attention mechanisms to weigh the importance of different words in the input sequence when processing a specific word.\n",
    "\n",
    "There are different types of LLMs, categorized based on their architecture and training objectives:\n",
    "\n",
    "*   **Generative LLMs:** These models are designed to generate new content, such as text, code, or images. Examples include GPT-3, LaMDA, and Stable Diffusion.\n",
    "*   **Discriminative LLMs:** These models are used for classification and analysis tasks, such as sentiment analysis or named entity recognition.\n",
    "*   **Hybrid LLMs:** These models combine aspects of both generative and discriminative models.\n",
    "\n",
    "Chunking is a crucial step in preparing text data for LLMs, especially for tasks like RAG. It involves breaking down a large text document into smaller, manageable pieces or \"chunks.\" The size and method of chunking can significantly impact the performance of LLM applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "uQ0xh1auuniJ"
   },
   "outputs": [],
   "source": [
    "sample_text2 = \"\"\"\n",
    "Chunking-the process of splitting documents into smaller, manageable pieces-directly impacts the quality of information retrieval, the relevance of generated responses, and ultimately, the user experience of your AI application. Most developers choose one chunking method and stick with it, but recent research suggests that the choice of chunking strategy can significantly impact performance.\n",
    "\n",
    "In this comprehensive analysis, we’ll explore various chunking strategies based on groundbreaking research from ChromaDB and other leading organizations in the field. We’ll examine each method’s strengths, weaknesses, and optimal use cases to help you make informed decisions for your RAG applications.\n",
    "\n",
    "Why Chunking Matters\n",
    "Before diving into specific strategies, it’s essential to understand why chunking is so crucial for RAG systems:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "f41443fb"
   },
   "outputs": [],
   "source": [
    "def print_chunks(chunks):\n",
    "    \"\"\"\n",
    "    Prints each chunk in a list of chunks.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): A list of text chunks.\n",
    "    \"\"\"\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    print(\"\\nChunks:\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"--- Chunk {i+1} ---\")\n",
    "        print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36181f47"
   },
   "source": [
    "### 3.1 Fixed-Size Chunking (LangChain)\n",
    "\n",
    "Fixed-size chunking is one of the most straightforward methods. It involves splitting the text into chunks of a predetermined size, often with a specified overlap between consecutive chunks. While simple, it can sometimes split sentences or paragraphs awkwardly, potentially losing context. It serves as a useful baseline to understand the basic concept of text splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57609812"
   },
   "source": [
    "We will use LangChain's `CharacterTextSplitter` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70eee26f",
    "outputId": "0ddf1fe5-b0be-4603-fad0-921c65e39138"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 9\n",
      "\n",
      "Chunks:\n",
      "--- Chunk 1 ---\n",
      "Large language models (LLMs) are a type of artificial intelligence algorithm that uses deep learning techniques and massively large data sets to understand, summarize, generate and predict new\n",
      "--- Chunk 2 ---\n",
      "and predict new content. The \"large\" in LLM refers to the vast amount of data these models are trained on — often terabytes of text and data — and the number of parameters they use to process\n",
      "--- Chunk 3 ---\n",
      "they use to process language, which can be billions.\n",
      "\n",
      "LLMs are built on transformer architectures, a neural network architecture introduced in 2017 that processes input data in a way that allows for\n",
      "--- Chunk 4 ---\n",
      "way that allows for parallelization, making training on massive data sets feasible. This architecture uses self-attention mechanisms to weigh the importance of different words in the input sequence\n",
      "--- Chunk 5 ---\n",
      "the input sequence when processing a specific word.\n",
      "\n",
      "There are different types of LLMs, categorized based on their architecture and training objectives:\n",
      "\n",
      "* **Generative LLMs:** These models are\n",
      "--- Chunk 6 ---\n",
      "These models are designed to generate new content, such as text, code, or images. Examples include GPT-3, LaMDA, and Stable Diffusion.\n",
      "* **Discriminative LLMs:** These models are used for\n",
      "--- Chunk 7 ---\n",
      "models are used for classification and analysis tasks, such as sentiment analysis or named entity recognition.\n",
      "* **Hybrid LLMs:** These models combine aspects of both generative and discriminative\n",
      "--- Chunk 8 ---\n",
      "and discriminative models.\n",
      "\n",
      "Chunking is a crucial step in preparing text data for LLMs, especially for tasks like RAG. It involves breaking down a large text document into smaller, manageable pieces\n",
      "--- Chunk 9 ---\n",
      "manageable pieces or \"chunks.\" The size and method of chunking can significantly impact the performance of LLM applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from rich.console import Console\n",
    "from rich.syntax import Syntax\n",
    "from rich.text import Text\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Define chunk size and overlap\n",
    "chunk_size = 200\n",
    "chunk_overlap = 20\n",
    "\n",
    "# Create the splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    ")\n",
    "\n",
    "# Split the text\n",
    "fixed_size_chunks = text_splitter.split_text(sample_text)\n",
    "print_chunks(fixed_size_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MILXayFxo29l"
   },
   "source": [
    "### 3.2 Recursive Character Splitting: A Smarter Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hFuLpYg1oI3h",
    "outputId": "d4e1bcc6-aa9c-41c8-d278-ecfd05fc4c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 14\n",
      "\n",
      "Chunks:\n",
      "--- Chunk 1 ---\n",
      "page_content='Large language models (LLMs) are a type of artificial intelligence algorithm that uses deep learning techniques and massively large data sets to'\n",
      "--- Chunk 2 ---\n",
      "page_content='large data sets to understand, summarize, generate and predict new content. The \"large\" in LLM refers to the vast amount of data these models are'\n",
      "--- Chunk 3 ---\n",
      "page_content='these models are trained on — often terabytes of text and data — and the number of parameters they use to process language, which can be billions.'\n",
      "--- Chunk 4 ---\n",
      "page_content='LLMs are built on transformer architectures, a neural network architecture introduced in 2017 that processes input data in a way that allows for'\n",
      "--- Chunk 5 ---\n",
      "page_content='way that allows for parallelization, making training on massive data sets feasible. This architecture uses self-attention mechanisms to weigh the'\n",
      "--- Chunk 6 ---\n",
      "page_content='to weigh the importance of different words in the input sequence when processing a specific word.'\n",
      "--- Chunk 7 ---\n",
      "page_content='There are different types of LLMs, categorized based on their architecture and training objectives:'\n",
      "--- Chunk 8 ---\n",
      "page_content='*   **Generative LLMs:** These models are designed to generate new content, such as text, code, or images. Examples include GPT-3, LaMDA, and Stable'\n",
      "--- Chunk 9 ---\n",
      "page_content='LaMDA, and Stable Diffusion.'\n",
      "--- Chunk 10 ---\n",
      "page_content='*   **Discriminative LLMs:** These models are used for classification and analysis tasks, such as sentiment analysis or named entity recognition.'\n",
      "--- Chunk 11 ---\n",
      "page_content='*   **Hybrid LLMs:** These models combine aspects of both generative and discriminative models.'\n",
      "--- Chunk 12 ---\n",
      "page_content='Chunking is a crucial step in preparing text data for LLMs, especially for tasks like RAG. It involves breaking down a large text document into'\n",
      "--- Chunk 13 ---\n",
      "page_content='text document into smaller, manageable pieces or \"chunks.\" The size and method of chunking can significantly impact the performance of LLM'\n",
      "--- Chunk 14 ---\n",
      "page_content='performance of LLM applications.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Create chunks\n",
    "chunks = text_splitter.create_documents([sample_text])\n",
    "print_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WgUTftMsFGp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqfV5xj1plhR"
   },
   "source": [
    "### 3.3 Content-Aware Chunking: Leveraging Document Structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27jrYJLCp4zK"
   },
   "source": [
    "Creating chunks within specific header groups is an intuitive idea. To address this challenge, we can use MarkdownHeaderTextSplitter. This will split a markdown file by a specified set of headers.\n",
    "\n",
    "For example, if we want to split this markdown:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "a9YrDfHcpbPp"
   },
   "outputs": [],
   "source": [
    "md = '# Foo\\n\\n ## Bar\\n\\nHi this is Jim  \\nHi this is Joe\\n\\n ## Baz\\n\\n Hi this is Molly'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLlHZKuAp-qq",
    "outputId": "742c7358-1391-4c82-af0d-95cec37e1a29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}, page_content='Hi this is Jim\\nHi this is Joe'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Baz'}, page_content='Hi this is Molly')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "# We can specify the headers to split on:\n",
    "\n",
    "headers = [(\"#\", \"Header 1\"),(\"##\", \"Header 2\"),(\"###\",\"Header 3\")]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers)\n",
    "md_header_splits = markdown_splitter.split_text(md)\n",
    "\n",
    "md_header_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EicOIBl3t7b9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYYfv_xWqZa0"
   },
   "source": [
    "By default, MarkdownHeaderTextSplitter strips headers being split on from the output chunk's content. This can be disabled by setting strip_headers = False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dUEMgy0kqS8W",
    "outputId": "75f1c707-6a2a-4916-df6c-28521dfbbc2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}, page_content='# Foo  \\n## Bar  \\nHi this is Jim\\nHi this is Joe'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Baz'}, page_content='## Baz  \\nHi this is Molly')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers,strip_headers=False)\n",
    "md_header_splits = markdown_splitter.split_text(md)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMHLcIzRqyYe"
   },
   "source": [
    "### 4.1 Semantic Chunking\n",
    "You will need the OpenAI API Key for running this\n",
    "Both LangChain and LlamaIndex, the leading frameworks for building LLM applications, offer robust implementations of semantic chunking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "p8ocecAmswo4"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "KEMUtaeZulxp"
   },
   "outputs": [],
   "source": [
    "docs = text_splitter.create_documents([sample_text,sample_text2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OybHquB6q9HO"
   },
   "source": [
    "### Langchain Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jsJTBswq3Pd",
    "outputId": "2069989e-eb13-4712-c309-3a1524d51415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of semantic chunks: 4\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from google.colab import userdata\n",
    "\n",
    "# Initialize the splitter with an embedding model and a breakpoint threshold type\n",
    "# Ensure the API key is stripped of any leading/trailing whitespace\n",
    "\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(api_key=OPENAI_API_KEY),\n",
    "    breakpoint_threshold_type=\"percentile\" # Other options: \"standard_deviation\", \"interquartile\", \"gradient\"\n",
    ")\n",
    "\n",
    "# Create semantically coherent document chunks\n",
    "# Assuming sample_text is defined elsewhere in the notebook\n",
    "docs = text_splitter.create_documents([sample_text,sample_text2])\n",
    "print(f\"Number of semantic chunks: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hmNA-EiWtJpD",
    "outputId": "ac6326cd-ecc3-48de-df97-2f45daf81b21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='\\nLarge language models (LLMs) are a type of artificial intelligence algorithm that uses deep learning techniques and massively large data sets to understand, summarize, generate and predict new content. The \"large\" in LLM refers to the vast amount of data these models are trained on — often terabytes of text and data — and the number of parameters they use to process language, which can be billions. LLMs are built on transformer architectures, a neural network architecture introduced in 2017 that processes input data in a way that allows for parallelization, making training on massive data sets feasible. This architecture uses self-attention mechanisms to weigh the importance of different words in the input sequence when processing a specific word. There are different types of LLMs, categorized based on their architecture and training objectives:\\n\\n*   **Generative LLMs:** These models are designed to generate new content, such as text, code, or images. Examples include GPT-3, LaMDA, and Stable Diffusion. *   **Discriminative LLMs:** These models are used for classification and analysis tasks, such as sentiment analysis or named entity recognition.'),\n",
       " Document(metadata={}, page_content='*   **Hybrid LLMs:** These models combine aspects of both generative and discriminative models. Chunking is a crucial step in preparing text data for LLMs, especially for tasks like RAG. It involves breaking down a large text document into smaller, manageable pieces or \"chunks.\" The size and method of chunking can significantly impact the performance of LLM applications. '),\n",
       " Document(metadata={}, page_content='\\nChunking-the process of splitting documents into smaller, manageable pieces-directly impacts the quality of information retrieval, the relevance of generated responses, and ultimately, the user experience of your AI application. Most developers choose one chunking method and stick with it, but recent research suggests that the choice of chunking strategy can significantly impact performance.'),\n",
       " Document(metadata={}, page_content='In this comprehensive analysis, we’ll explore various chunking strategies based on groundbreaking research from ChromaDB and other leading organizations in the field. We’ll examine each method’s strengths, weaknesses, and optimal use cases to help you make informed decisions for your RAG applications. Why Chunking Matters\\nBefore diving into specific strategies, it’s essential to understand why chunking is so crucial for RAG systems:\\n')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uB_F9MNdrqU6"
   },
   "source": [
    "### Llama index Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131,
     "referenced_widgets": [
      "65e9c7dd00154567ac59f8b46c0574bb",
      "5eec51d1daf24d199428fe58e1a7bb9c",
      "a5c35f9a078647b181222a996d62af45",
      "897a69175f4640849a9b2b3114e604ab",
      "fe64ca89bf23440e832f576848629d71",
      "716df67669ae4d349e780fc626451aad",
      "985bc519996740d78febe232d90c7fed",
      "10b8e73943a74f498225fc4cec2f813c",
      "39f6988be61b46b991a08e212ee730ff",
      "170c2d93b4154828827d939d74ffb5bf",
      "7e680ec67cd74ca6a8c4188da0b293b5",
      "11cc2cc13eb94d87bb31a39e359e2c28",
      "7d0180c158d9474895f4f67ad973e0ff",
      "2d677483417e434e8c2ffe726eefecfa",
      "5ef40e10dcb54729a1db577c1b9dfa66",
      "7f5e5f7841c14f788790f47366ab154d",
      "4e86bfb019b1466187625a2de47f0a9c",
      "afd459fbcbb646c0977926b6b854170f",
      "429968c0a1604d84972f97ec593a593a",
      "01b14b4eb9774b35889e108882610f8c",
      "722683b1f3424f6f8681a62a3400a994",
      "dfc4a09027344725a73f34f65c6277ff",
      "7777a44fe5c7439d84991fd2ab85356c",
      "3499bff822414b89b919df751d2c4f28",
      "d88a89dc30bc46c2af8b8ad5a5c2398f",
      "9cffaa5c1fa14b8ba850c988ffd29995",
      "464a6008ab5c4ceb8994227a4b358e29",
      "55f097fbe413421db0c88030eba2c946",
      "00b51d2c37194545943eafb4438495c4",
      "5bdac7852a2743d1a35ceff22ec25904",
      "dfaf4afa731647c58f8af5af0d77f40e",
      "906e79dca7c840d78ff873f4fd192af9",
      "56ed841c40bb486191cf92d8f194f2d7"
     ]
    },
    "id": "MS3nRc0rW9o",
    "outputId": "2065259c-5579-4b8a-e0bd-41425d84b938"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e9c7dd00154567ac59f8b46c0574bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11cc2cc13eb94d87bb31a39e359e2c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7777a44fe5c7439d84991fd2ab85356c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of semantic nodes: 4\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Document\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=OpenAIEmbedding(api_key=OPENAI_API_KEY)\n",
    ")\n",
    "\n",
    "text_list = [sample_text, sample_text2]\n",
    "documents = [Document(text=t) for t in text_list]\n",
    "nodes = splitter.get_nodes_from_documents(documents,show_progress=True)\n",
    "print(f\"Number of semantic nodes: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fN952GSzr3Pb"
   },
   "source": [
    "### 4.2 Hierarchical & Relational Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "HSlwQVrXrWXQ"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# This text splitter is used to create the child documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "# This text splitter is used to create the parent documents\n",
    "# It should create larger chunks than the child splitter\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "\n",
    "# The vector store to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"split_parents\", embedding_function=OpenAIEmbeddings(api_key=OPENAI_API_KEY))\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "# Add documents to the retriever, which handles the splitting and storage\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B00efQI5v53f",
    "outputId": "ceac775a-966b-4242-8c31-94d001c0da66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParentDocumentRetriever(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x78ce843b0bd0>, docstore=<langchain_core.stores.InMemoryStore object at 0x78ce87a9bc10>, search_kwargs={}, child_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x78ce883a7ed0>, parent_splitter=<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x78ce87abfc50>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
